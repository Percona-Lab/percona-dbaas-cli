#!/bin/bash

# GIT_COMMIT=$(git rev-parse HEAD)
# GIT_BRANCH=${VERSION:-$(git rev-parse --abbrev-ref HEAD | sed -e 's^/^-^g; s^[.]^-^g;' | tr '[:upper:]' '[:lower:]')}
# IMAGE=${IMAGE:-"perconalab/percona-server-mongodb-operator:${GIT_BRANCH}"}
# IMAGE_MONGOD=${IMAGE_MONGOD:-"perconalab/percona-server-mongodb-operator:master-mongod4.0"}
# IMAGE_BACKUP=${IMAGE_BACKUP:-"perconalab/percona-server-mongodb-operator:master-backup"}
tmp_dir=$(mktemp -d)
sed=$(which gsed || which sed)
date=$(which gdate || which date)

test_name=$(basename ${TEST_DIR})
namespace="${test_name}-${RANDOM}"
conf_dir=$(realpath ${TEST_DIR}/../conf || :)
SRC_DIR=$(realpath ${TEST_DIR}/../..)
if oc projects 2>&1 | egrep -q 'You have access to the following projects|You are not a member of any projects|You have one project on this server'; then
    OPENSHIFT=1
fi

build() {

    if [[ -z $(which go) ]]; then
        echo "Can't find go compiler. Please check wheather it's installed and present in PATH"
    fi

    pushd ${SRC_DIR}/cmd/percona-dbaas > /dev/null 2>&1
    go build
    popd > /dev/null 2>&1
}

check_build() {
    export PATH=${SRC_DIR}/cmd/percona-dbaas:${PATH}
    
    if [[ ! -f ${SRC_DIR}/cmd/percona-dbaas/percona-dbaas ]]; then
        build
    fi
}

create_namespace() {
    local namespace="$1"
    if [ "$OPENSHIFT" == 1 ]; then
        oc delete project "$namespace" && sleep 40 || :
        oc new-project "$namespace"
        oc project "$namespace"
        oc adm policy add-scc-to-user hostaccess -z default || :
    else
        kubectl delete namespace "$namespace" || :
        wait_for_delete "namespace/$namespace"
        kubectl create namespace "$namespace"
        kubectl config set-context $(kubectl config current-context) --namespace="$namespace"
    fi
}

get_operator_pod() {
    kubectl get pods \
        --selector=name=percona-server-mongodb-operator \
        -o 'jsonpath={.items[].metadata.name}'
}

wait_pod() {
    : waiting pod
    local pod=$1

    set +o xtrace
    retry=0
    echo -n $pod
    #until kubectl get pod/$pod -o jsonpath='{.status.phase}' 2>/dev/null | grep 'Running'; do
    until kubectl get pod/$pod -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep 'true'; do
        sleep 1
        echo -n .
        let retry+=1
        if [ $retry -ge 120 ]; then
            kubectl describe pod/$pod
            kubectl logs $pod
            kubectl logs $(get_operator_pod) \
                | grep -v 'level=info' \
                | grep -v 'level=debug' \
                | grep -v 'Getting tasks for pod' \
                | grep -v 'Getting pods from source' \
                | tail -100
            echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
            exit 1
        fi
    done
    set -o xtrace
}

wait_cluster_consistency() {
    cluster_name=$1
    cluster_size=$2
    until [[ "$(kubectl get psmdb "${cluster_name}" -o jsonpath='{.status.state}')" == "ready" \
             && "$(kubectl get psmdb "${cluster_name}" -o jsonpath='{.status.replsets.rs0.ready}')" == "${cluster_size}" ]]; do
        echo 'waiting for cluster readyness'
        sleep 20
    done
}
# wait_cron() {
#     local backup=$1

#     set +o xtrace
#     retry=0
#     echo -n $backup
#     until kubectl get cronjob/$backup -o jsonpath='{.status.lastScheduleTime}' 2>/dev/null | grep 'T'; do
#         sleep 1
#         echo -n .
#         let retry+=1
#         if [ $retry -ge 360 ]; then
#             kubectl logs $(get_operator_pod) \
#                 | grep -v 'level=info' \
#                 | grep -v 'level=debug' \
#                 | grep -v 'Getting tasks for pod' \
#                 | grep -v 'Getting pods from source' \
#                 | tail -100
#             echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
#             exit 1
#         fi
#     done
#     set -o xtrace
# }

# wait_backup() {
#     local backup_name=$1

#     set +o xtrace
#     retry=0
#     echo -n $backup_name
#     until [ "$(kubectl get psmdb-backup $backup_name -o jsonpath='{.status.state}')" == "ready" ]; do
#         sleep 1
#         echo -n .
#         let retry+=1
#         if [ $retry -ge 120 ]; then
#             kubectl logs $(get_operator_pod) \
#                 | grep -v 'level=info' \
#                 | grep -v 'level=debug' \
#                 | grep -v 'Getting tasks for pod' \
#                 | grep -v 'Getting pods from source' \
#                 | tail -100
#             echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
#             exit 1
#         fi
#     done
#     echo
#     set -o xtrace
# }

# run_restore() {
#     local backup_name=$1
#     let last_pod="$2-1" || :

#     run_mongo \
#         'use myApp\n db.test.insert({ x: 100501 })' \
#         "myApp:myPass@$cluster.$namespace"

#     for i in $(seq 0 $last_pod); do
#         compare_mongo_cmd "find" "myApp:myPass@$cluster-${i}.$cluster.$namespace" "-2nd"
#     done

#     cat $TEST_DIR/conf/restore.yml \
#         | $sed -e "s/name:/name: restore-$backup_name/" \
#         | $sed -e "s/backupName:/backupName: $backup_name/" \
#         | kubectl apply -f -
# }

# wait_restore() {
#     wait_restore_object "${1}"
#     echo
#     set -o xtrace

#     simple_data_check "${cluster}" "${2}"
# }

# simple_data_check() {
#     cluster_name=${1}
#     let last_pod="$2-1" || :
#     for i in $(seq 0 $last_pod); do
#         compare_mongo_cmd "find" "myApp:myPass@${cluster_name}-${i}.${cluster_name}.$namespace"
#     done
# }

# wait_restore_object() {
#     local backup_name=$1

#     set +o xtrace
#     retry=0
#     echo -n $backup_name
#     until [ "$(kubectl get psmdb-restore restore-$backup_name -o jsonpath='{.status.state}')" == "ready" ]; do
#         sleep 1
#         echo -n .
#         let retry+=1
#         if [ $retry -ge 120 ]; then
#             kubectl logs $(get_operator_pod) \
#                 | grep -v 'level=info' \
#                 | grep -v 'level=debug' \
#                 | grep -v 'Getting tasks for pod' \
#                 | grep -v 'Getting pods from source' \
#                 | tail -100
#             echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
#             exit 1
#         fi
#     done
# }

# deploy_operator() {
#     desc 'start operator'

#     kubectl apply -f ${SRC_DIR}/deploy/crd.yaml || :
#     kubectl apply -f ${SRC_DIR}/deploy/rbac.yaml

#     cat ${SRC_DIR}/deploy/operator.yaml \
#         | sed -e "s^image: .*^image: ${IMAGE}^" \
#         | kubectl apply -f -

#     sleep 2

#     wait_pod $(get_operator_pod)
# }

deploy_helm() {
    local namespace="$1"
    if [ "$OPENSHIFT" == 1 ]; then
        export TILLER_NAMESPACE=tiller
        HELM_VERSION=$(helm version -c | $sed -re 's/.*SemVer:"([^"]+)".*/\1/')
        oc new-project tiller || :
        oc project tiller
        oc process -f https://github.com/openshift/origin/raw/master/examples/helm/tiller-template.yaml -p TILLER_NAMESPACE="tiller" -p HELM_VERSION=$HELM_VERSION | oc apply -f -
    else
        kubectl --namespace kube-system create sa tiller || :
        kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller || :
        helm init --service-account tiller
        kubectl config set-context $(kubectl config current-context) --namespace="kube-system"
    fi

    tiller_pod=$(
        kubectl get pods \
            --selector=name=tiller \
            -o 'jsonpath={.items[].metadata.name}'
    )
    wait_pod $tiller_pod

    if [ "$OPENSHIFT" == 1 ]; then
        oc project "$namespace"
        oc policy add-role-to-user edit "system:serviceaccount:tiller:tiller"
    else
        kubectl config set-context $(kubectl config current-context) --namespace="$namespace"
    fi
}

deploy_minio() {
    desc 'install Minio'
    helm del --purge minio-service || :
    # kubectl delete pvc minio-service --force
    helm install \
        --name minio-service \
        --set accessKey=some-access-key \
        --set secretKey=some-secret-key \
        --set service.type=ClusterIP \
        --set configPath=/tmp/.minio/ \
        --set persistence.size=2G \
        --set environment.MINIO_REGION=us-east-1 \
        --set environment.MINIO_HTTP_TRACE=/tmp/trace.log \
        stable/minio
    MINIO_POD=$(kubectl get pods --selector=release=minio-service -o 'jsonpath={.items[].metadata.name}')
    wait_pod "${MINIO_POD}"

    # create bucket
    kubectl run -i --rm aws-cli --image=perconalab/awscli --restart=Never -- \
        bash -c 'AWS_ACCESS_KEY_ID=some-access-key AWS_SECRET_ACCESS_KEY=some-secret-key AWS_DEFAULT_REGION=us-east-1 \
        /usr/bin/aws --endpoint-url http://minio-service:9000 s3 mb s3://operator-testing'
}

# wait_for_running() {
#     local name="$1"
#     let last_pod="$(($2-1))" || :

#     for i in $(seq 0 $last_pod); do
#         wait_pod ${name}-${i}
#     done
#     sleep 10
# }

wait_for_delete() {
    local res="$1"

    set +o xtrace
    echo -n "$res - "
    retry=0
    until (kubectl get $res || :) 2>&1 | grep NotFound; do
        sleep 1
        echo -n .
        let retry+=1
        if [ $retry -ge 60 ]; then
            kubectl logs $(get_operator_pod) \
                | grep -v 'level=info' \
                | grep -v 'level=debug' \
                | grep -v 'Getting tasks for pod' \
                | grep -v 'Getting pods from source' \
                | tail -100
            echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
            exit 1
        fi
    done
    set -o xtrace
}

compare_kubectl() {
    local resource="$1"
    local postfix="$2"
    local expected_result=${TEST_DIR}/compare/${resource//\//_}${postfix}.yml
    local new_result="${tmp_dir}/${resource//\//_}.yml"

    if [ "$OPENSHIFT" = 1 -a -f ${expected_result//.yml/-oc.yml} ]; then
        expected_result=${expected_result//.yml/-oc.yml}
    fi

    kubectl get -o yaml ${resource} \
        | egrep -v "namespace:|uid:|resourceVersion:|selfLink:|creationTimestamp:|image:|clusterIP:|dataSource:|procMount:" \
        | egrep -v "^  storageClassName:|finalizers:|kubernetes.io/pvc-protection|volumeName:|storage-provisioner:|status: \{\}|volumeMode: Filesystem" \
        | egrep -v "percona.com/.*-hash:|control-plane.alpha.kubernetes.io/leader:|volume.kubernetes.io/selected-node:" \
        | egrep -v "healthCheckNodePort:|nodePort:" \
        | $sed -e '/^status:$/,+100500d' \
        | $sed -e '/NAMESPACE/,+1d' \
        | $sed -e '/PBM_AGENT_SERVER_ADDRESS/,+1d' \
        | $sed -e '/name: suffix/,+1d' \
        > ${new_result}
    diff -u ${expected_result} ${new_result}
}

run_mongo() {
    local command="$1"
    local uri="$2"
    local driver=${3:-mongodb+srv}
    local suffix=${4:-.svc.cluster.local}
    local client_container=$(kubectl get pods --selector=name=psmdb-client -o 'jsonpath={.items[].metadata.name}')

    kubectl exec ${client_container} -- \
        bash -c "printf '$command\n' | mongo $driver://$uri$suffix/admin?ssl=false\&replicaSet=rs0"
}

# get_service_ip() {
#     local service=$1
#     if [ "$(kubectl get psmdb/${service/-rs0*/} -o 'jsonpath={.spec.replsets[].expose.enabled}')" != "true" ]; then
#         echo -n $service.${service/-rs0*/}-rs0
#         return
#     fi
#     while (kubectl get service/$service -o 'jsonpath={.spec.type}' 2>&1 || : ) | grep -q NotFound; do
#         sleep 1
#     done
#     if [ "$(kubectl get service/$service -o 'jsonpath={.spec.type}')" = "ClusterIP" ]; then
#         kubectl get service/$service -o 'jsonpath={.spec.clusterIP}'
#         return
#     fi
#     until kubectl get service/$service -o 'jsonpath={.status.loadBalancer.ingress[]}' 2>&1 | egrep -q "hostname|ip"; do
#         sleep 1
#     done
#     kubectl get service/$service -o 'jsonpath={.status.loadBalancer.ingress[].ip}'
#     kubectl get service/$service -o 'jsonpath={.status.loadBalancer.ingress[].hostname}'
# }

compare_mongo_cmd() {
    local command="$1"
    local uri="$2"
    local postfix="$3"
    local suffix="$4"

    run_mongo "use myApp\n db.test.${command}()" "$uri" "mongodb" "$suffix" \
        | egrep -v 'I NETWORK|W NETWORK|Error saving history file|Percona Server for MongoDB|connecting to:|Unable to reach primary for set|Implicit session:|versions do not match' \
        | $sed -re 's/ObjectId\("[0-9a-f]+"\)//; s/-[0-9]+.svc/-xxx.svc/' \
        > $tmp_dir/${command}
    diff ${TEST_DIR}/compare/${command}${postfix}.json $tmp_dir/${command}
}

# get_mongo_primary_endpoint() {
#     local uri="$1"

#     run_mongo 'db.isMaster().me' "$uri" "mongodb" ":27017" \
#         | egrep -v "Time|Percona Server for MongoDB|bye|BinData|NumberLong|connecting to|Error saving history file|I NETWORK|W NETWORK|Implicit session:|versions do not match" \
#         | sed -e 's^20[0-9][0-9]-[0-9][0-9]-[0-9][0-9]T[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\.[0-9][0-9][0-9]+[0-9][0-9][0-9][0-9]^^' \
#         | grep ":27017$"
# }

# get_mongo_primary() {
#     local uri="$1"
#     local cluster="$2"

#     endpoint=$(get_mongo_primary_endpoint $uri)
#     if [[ "$endpoint" =~ ".$cluster" ]]; then
#         echo $endpoint \
#             | cut -d . -f 1
#     else
#         kubectl get service -o wide \
#             | grep " ${endpoint/:*/} " \
#             | awk '{print$1}'
#     fi
# }

# compare_mongo_user() {
#     local uri="$1"
#     local user=$(echo $uri | cut -d : -f 1)
#     local expected_result=${TEST_DIR}/compare/$user.json

#     if [[ "$IMAGE_MONGOD" =~ 4\.0$ ]] && [ -f ${TEST_DIR}/compare/$user-40.json ]; then
#         expected_result=${TEST_DIR}/compare/$user-40.json
#     fi

#     run_mongo 'db.runCommand({connectionStatus:1,showPrivileges:true})' "$uri" \
#         | egrep -v "Time|Percona Server for MongoDB|bye|BinData|NumberLong|connecting to|Error saving history file|I NETWORK|W NETWORK|Implicit session:|versions do not match" \
#         | sed -e 's^20[0-9][0-9]-[0-9][0-9]-[0-9][0-9]T[0-9][0-9]:[0-9][0-9]:[0-9][0-9]\.[0-9][0-9][0-9]+[0-9][0-9][0-9][0-9]^^' \
#         | $sed -e '/"ok" : 1/,+4d' \
#         > $tmp_dir/$user.json
#     diff $expected_result $tmp_dir/$user.json
# }

# start_gke() {
#     gcloud container clusters create operator-testing-$RANDOM --zone europe-west3-c --project cloud-dev-112233 --preemptible --cluster-version 1.11
# }

# get_pumba() {
#     kubectl get pods \
#         --selector=name=pumba \
#         -o 'jsonpath={.items[].metadata.name}'
# }

# run_pumba() {
#     local cmd="$*"
#     kubectl exec -it "$(get_pumba)" -- /pumba -l info ${cmd}
# }

deploy_cert_manager() {
    kubectl create namespace cert-manager || :
    kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true || :
    kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.7/deploy/manifests/cert-manager.yaml --validate=false || : 2>/dev/null
}

destroy() {
    local namespace="$1"

    kubectl logs $(get_operator_pod) \
        | grep -v 'level=info' \
        | grep -v 'level=debug' \
        | grep -v 'Getting tasks for pod' \
        | grep -v 'Getting pods from source' \
        | grep -v 'the object has been modified' \
        | grep -v 'get backup status: Job.batch' \
        | $sed -r 's/"ts":[0-9.]+//; s^limits-[0-9.]+/^^g' \
        | sort -u \
        | tee $tmp_dir/operator.log

    #TODO: maybe will be enabled later
    #diff $TEST_DIR/compare/operator.log $tmp_dir/operator.log

    kubectl delete psmdb --all
    kubectl delete psmdb-backup --all || :
    kubectl delete psmdb-recovery --all || :

    kubectl delete -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.7/deploy/manifests/cert-manager.yaml 2>/dev/null || :
    if [ "$OPENSHIFT" == 1 ]; then
        oc delete --grace-period=0 --force=true project "$namespace"
    else
        kubectl delete --grace-period=0 --force=true namespace "$namespace"
    fi
    rm -rf ${tmp_dir}
}

desc() {
    set +o xtrace
    local msg="$@"
    printf "\n\n-----------------------------------------------------------------------------------\n"
    printf "$msg"
    printf "\n-----------------------------------------------------------------------------------\n\n"
    set -o xtrace
}

# get_backup_dest() {
#     local backup_name=$1

#     kubectl get psmdb-backup $backup_name -o jsonpath='{.status.destination}' \
#         | sed -e 's/.json$//'
# }

# get_service_endpoint() {
#     local service=$1

#     local hostname=$(
#         kubectl get service/$service -o json \
#             | jq '.status.loadBalancer.ingress[].hostname' \
#             | sed -e 's/^"//; s/"$//;'
#     )
#     if [ -n "$hostname" -a "$hostname" != "null" ]; then
#         echo $hostname
#         return
#     fi

#     local ip=$(
#         kubectl get service/$service -o json \
#             | jq '.status.loadBalancer.ingress[].ip' \
#             | sed -e 's/^"//; s/"$//;'
#     )
#     if [ -n "$ip" -a "$ip" != "null" ]; then
#         echo $ip
#         return
#     fi

#     exit 1
# }

# get_metric_values() {
#     local metric=$1
#     local instance=$2
#     local start=$($date -u "+%s" -d "-1 minute")
#     local end=$($date -u "+%s")
#     local endpoint=$(get_service_endpoint monitoring-service)
#     curl -s -k "https://$endpoint/graph/api/datasources/proxy/1/api/v1/query_range?query=$metric%7Binstance%3D%22$instance%22%7D&start=$start&end=$end&step=60" \
#         | jq '.data.result[0].values[][1]' \
#         | grep '^"[0-9]'
# }

# get_qan_values() {
#     local instance=$1
#     local start=$($date -u "+%Y-%m-%dT%H:%M:%S" -d "-30 minute")
#     local end=$($date -u "+%Y-%m-%dT%H:%M:%S")
#     local endpoint=$(get_service_endpoint monitoring-service)

#     local uuid=$(
#         curl -s -k "https://$endpoint/qan-api/instances?deleted=no" \
#             | jq '.[] | select(.Subsystem == "mongo" and .Name == "'$instance'") | .UUID' \
#             | sed -e 's/^"//; s/"$//;'
#     )

#     curl -s -k "https://$endpoint/qan-api/qan/profile/$uuid?begin=$start&end=$end&offset=0" \
#         | jq '.Query[].Fingerprint'
# }

# apply_cluster() {
#     local file=$1

#     $sed -e "s#image: .*-mongod[34].*#image: $IMAGE_MONGOD#" "$file" \
#         | $sed -e "s#image:\$#image: $IMAGE_MONGOD#" \
#         | $sed -e "s#image: .*-backup.*\$#image: $IMAGE_BACKUP#" \
#         | kubectl apply -f -
# }

# spinup_psmdb() {
#     local cluster=$1
#     local config=$2
#     local size="${3:-3}"

#     desc 'create first PSMDB cluster'
#     apply_cluster $config

#     desc 'check if Pod is started'
#     wait_for_running "${cluster}" "$size"
#     sleep 20

#     compare_kubectl "statefulset/${cluster}"

#     desc 'write data'

#     run_mongo 'db.createUser({user: "myApp", pwd: "myPass", roles: [{ db: "myApp", role: "readWrite" }]})' \
#               "userAdmin:userAdmin123456@${cluster}.${namespace}"

#     run_mongo 'use myApp\n db.test.insert({ x: 100500 })' "myApp:myPass@${cluster}.${namespace}"
# }