#!/bin/bash

exec 5>&2
BASH_XTRACEFD="5"

tmp_dir=$(mktemp -d)
sed=$(which gsed || which sed)
date=$(which gdate || which date)

test_name=$(basename ${TEST_DIR})
namespace="${test_name}-${RANDOM}"
conf_dir=$(realpath ${TEST_DIR}/../conf || :)
SRC_DIR=$(realpath ${TEST_DIR}/../..)
if oc projects 2>&1 | egrep -q 'You have access to the following projects|You are not a member of any projects|You have one project on this server'; then
    OPENSHIFT=1
fi

create_namespace() {
    local namespace="$1"
    if [ "$OPENSHIFT" == 1 ]; then
        oc delete project "$namespace" && sleep 40 || :
        oc new-project "$namespace"
        oc project "$namespace"
        oc adm policy add-scc-to-user hostaccess -z default || :
    else
        kubectl_bin delete namespace "$namespace" || :
        wait_for_delete "namespace/$namespace"
        kubectl_bin create namespace "$namespace"
        kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="$namespace"
    fi
}

get_operator_pod() {
    kubectl_bin get pods \
        --selector=name=percona-server-mongodb-operator \
        -o 'jsonpath={.items[].metadata.name}'
}

wait_pod() {
    : waiting pod
    local pod=$1

    set +o xtrace
    retry=0
    echo -n $pod
    #until kubectl_bin get pod/$pod -o jsonpath='{.status.phase}' 2>/dev/null | grep 'Running'; do
    until kubectl_bin get pod/$pod -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null | grep 'true'; do
        sleep 1
        echo -n .
        let retry+=1
        if [ $retry -ge 120 ]; then
            kubectl_bin describe pod/$pod
            kubectl_bin logs $pod
            kubectl_bin logs $(get_operator_pod) \
                | grep -v 'level=info' \
                | grep -v 'level=debug' \
                | grep -v 'Getting tasks for pod' \
                | grep -v 'Getting pods from source' \
                | tail -100
            echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
            exit 1
        fi
    done
    set -o xtrace
}

wait_cluster_consistency() {
    cluster_name=$1
    cluster_size=$2
    object_type=${3:-psmdb}

    case ${object_type} in
        psmdb )
            until [[ "$(kubectl_bin get "${object_type}" "${cluster_name}" -o jsonpath='{.status.state}')" == "ready" \
                 && "$(kubectl_bin get "${object_type}" "${cluster_name}" -o jsonpath='{.status.replsets.rs0.ready}')" == "${cluster_size}" ]]; do
                echo 'waiting for cluster readyness'
                sleep 20
            done
            ;;
        pxc )
            until [[ "$(kubectl_bin get "${object_type}" "${cluster_name}" -o jsonpath='{.status.state}')" == "ready" \
                 && "$(kubectl_bin get "${object_type}" "${cluster_name}" -o jsonpath='{.status.proxysql.ready}')" == "${cluster_size}" \
                 && "$(kubectl_bin get "${object_type}" "${cluster_name}" -o jsonpath='{.status.pxc.ready}')" == "${cluster_size}" ]]; do
                echo 'waiting for cluster readyness'
                sleep 20
            done
            ;;
    esac
}

deploy_helm() {
    local namespace="$1"
    if [ "$OPENSHIFT" == 1 ]; then
        export TILLER_NAMESPACE=tiller
        HELM_VERSION=$(helm version -c | $sed -re 's/.*SemVer:"([^"]+)".*/\1/')
        oc new-project tiller || :
        oc project tiller
        oc process -f https://github.com/openshift/origin/raw/master/examples/helm/tiller-template.yaml -p TILLER_NAMESPACE="tiller" -p HELM_VERSION=$HELM_VERSION | oc apply -f -
    else
        kubectl_bin --namespace kube-system create sa tiller || :
        kubectl_bin create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller || :
        helm init --service-account tiller
        kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="kube-system"
    fi

    tiller_pod=$(
        kubectl_bin get pods \
            --selector=name=tiller \
            -o 'jsonpath={.items[].metadata.name}'
    )
    wait_pod $tiller_pod

    if [ "$OPENSHIFT" == 1 ]; then
        oc project "$namespace"
        oc policy add-role-to-user edit "system:serviceaccount:tiller:tiller"
    else
        kubectl_bin config set-context $(kubectl_bin config current-context) --namespace="$namespace"
    fi
}

deploy_minio() {
    desc 'install Minio'
    helm del --purge minio-service || :
    # kubectl_bin delete pvc minio-service --force
    helm install \
        --name minio-service \
        --set accessKey=some-access-key \
        --set secretKey=some-secret-key \
        --set service.type=ClusterIP \
        --set configPath=/tmp/.minio/ \
        --set persistence.size=2G \
        --set environment.MINIO_REGION=us-east-1 \
        --set environment.MINIO_HTTP_TRACE=/tmp/trace.log \
        stable/minio
    MINIO_POD=$(kubectl_bin get pods --selector=release=minio-service -o 'jsonpath={.items[].metadata.name}')
    wait_pod "${MINIO_POD}"

    # create bucket
    kubectl_bin run -i --rm aws-cli --image=perconalab/awscli --restart=Never -- \
        bash -c 'AWS_ACCESS_KEY_ID=some-access-key AWS_SECRET_ACCESS_KEY=some-secret-key AWS_DEFAULT_REGION=us-east-1 \
        /usr/bin/aws --endpoint-url http://minio-service:9000 s3 mb s3://operator-testing'
}

wait_for_delete() {
    local res="$1"

    set +o xtrace
    echo -n "$res - "
    retry=0
    until (kubectl_bin get $res || :) 2>&1 | grep NotFound; do
        sleep 1
        echo -n .
        let retry+=1
        if [ $retry -ge 60 ]; then
            kubectl_bin logs $(get_operator_pod) \
                | grep -v 'level=info' \
                | grep -v 'level=debug' \
                | grep -v 'Getting tasks for pod' \
                | grep -v 'Getting pods from source' \
                | tail -100
            echo max retry count $retry reached. something went wrong with operator or kubernetes cluster
            exit 1
        fi
    done
    set -o xtrace
}

compare_kubectl() {
    local resource="$1"
    local postfix="$2"
    local expected_result=${TEST_DIR}/compare/${resource//\//_}${postfix}.yml
    local new_result="${tmp_dir}/${resource//\//_}.yml"

    if [ "$OPENSHIFT" = 1 -a -f ${expected_result//.yml/-oc.yml} ]; then
        expected_result=${expected_result//.yml/-oc.yml}
    fi

    kubectl_bin get -o yaml ${resource} \
        | egrep -v "namespace:|uid:|resourceVersion:|selfLink:|creationTimestamp:|image:|clusterIP:|dataSource:|procMount:" \
        | egrep -v "^  storageClassName:|finalizers:|kubernetes.io/pvc-protection|volumeName:|storage-provisioner:|status: \{\}|volumeMode: Filesystem" \
        | egrep -v "percona.com/.*-hash:|control-plane.alpha.kubernetes.io/leader:|volume.kubernetes.io/selected-node:" \
        | egrep -v "healthCheckNodePort:|nodePort:" \
        | $sed -e '/^status:$/,+100500d' \
        | $sed -e '/NAMESPACE/,+1d' \
        | $sed -e '/PBM_AGENT_SERVER_ADDRESS/,+1d' \
        | $sed -e '/name: suffix/,+1d' \
        > ${new_result}
    diff -u ${expected_result} ${new_result}
}

run_mongo() {
    local command="$1"
    local uri="$2"
    local driver=${3:-mongodb+srv}
    local suffix=${4:-.svc.cluster.local}
    local client_container=$(kubectl_bin get pods --selector=name=psmdb-client -o 'jsonpath={.items[].metadata.name}')

    kubectl_bin exec ${client_container} -- \
        bash -c "printf '$command\n' | mongo $driver://$uri$suffix/admin?ssl=false\&replicaSet=rs0"
}

compare_mongo_cmd() {
    local command="$1"
    local uri="$2"
    local postfix="$3"
    local suffix="$4"

    run_mongo "use myApp\n db.test.${command}()" "$uri" "mongodb" "$suffix" \
        | egrep -v 'I NETWORK|W NETWORK|Error saving history file|Percona Server for MongoDB|connecting to:|Unable to reach primary for set|Implicit session:|versions do not match' \
        | $sed -re 's/ObjectId\("[0-9a-f]+"\)//; s/-[0-9]+.svc/-xxx.svc/' \
        > $tmp_dir/${command}
    diff ${TEST_DIR}/compare/${command}${postfix}.json $tmp_dir/${command}
}


deploy_cert_manager() {
    kubectl_bin create namespace cert-manager || :
    kubectl_bin label namespace cert-manager certmanager.k8s.io/disable-validation=true || :
    kubectl_bin apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.7/deploy/manifests/cert-manager.yaml --validate=false || : 2>/dev/null
}

destroy() {
    local namespace="$1"

    kubectl_bin logs $(get_operator_pod) \
        | grep -v 'level=info' \
        | grep -v 'level=debug' \
        | grep -v 'Getting tasks for pod' \
        | grep -v 'Getting pods from source' \
        | grep -v 'the object has been modified' \
        | grep -v 'get backup status: Job.batch' \
        | $sed -r 's/"ts":[0-9.]+//; s^limits-[0-9.]+/^^g' \
        | sort -u \
        | tee $tmp_dir/operator.log

    #TODO: maybe will be enabled later
    #diff $TEST_DIR/compare/operator.log $tmp_dir/operator.log

    kubectl_bin delete psmdb --all
    kubectl_bin delete psmdb-backup --all || :
    kubectl_bin delete psmdb-recovery --all || :

    kubectl_bin delete -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.7/deploy/manifests/cert-manager.yaml 2>/dev/null || :
    if [ "$OPENSHIFT" == 1 ]; then
        oc delete --grace-period=0 --force=true project "$namespace"
    else
        kubectl_bin delete --grace-period=0 --force=true namespace "$namespace"
    fi
    rm -rf ${tmp_dir}
}

desc() {
    set +o xtrace
    local msg="$@"
    printf "\n\n-----------------------------------------------------------------------------------\n"
    printf "$msg"
    printf "\n-----------------------------------------------------------------------------------\n\n"
    set -o xtrace
}

run_mysql() {
    local command="$1"
    local uri="$2"

    client_pod=$(get_client_pod)
    wait_pod $client_pod 1>&2
    kubectl_bin exec $client_pod -- \
        bash -c "printf '$command\n' | mysql -sN $uri" 2>&1 \
        | sed -e 's/mysql: //' \
        | (grep -v 'Using a password on the command line interface can be insecure.' || :)
}

get_client_pod() {
    kubectl_bin get pods \
        --selector=name=pxc-client \
        -o 'jsonpath={.items[].metadata.name}'
}

compare_mysql_cmd() {
    local command_id="$1"
    local command="$2"
    local uri="$3"
    local postfix="$4"

    run_mysql "$command" "$uri" \
        > $tmp_dir/${command_id}.sql
    diff ${TEST_DIR}/compare/${command_id}${postfix}.sql $tmp_dir/${command_id}.sql
}

kubectl_bin() {
    local LAST_OUT="$(mktemp)"
    local LAST_ERR="$(mktemp)"
    local exit_status=0
    for i in $(seq 0 2); do
        kubectl "$@" 1>"$LAST_OUT" 2>"$LAST_ERR"
        exit_status=$?
        [[ ${-/x} != $- ]] && echo "--- $i stdout" | cat - "$LAST_OUT" >&$BASH_XTRACEFD
        [[ ${-/x} != $- ]] && echo "--- $i stderr" | cat - "$LAST_ERR" >&$BASH_XTRACEFD
        if [[ ${exit_status} != 0 ]]; then
            sleep "$((timeout * i))"
        else
            cat "$LAST_OUT"
            cat "$LAST_ERR" >&2
            rm "$LAST_OUT" "$LAST_ERR"
            return ${exit_status}
        fi
    done
    cat "$LAST_OUT"
    cat "$LAST_ERR" >&2
    rm "$LAST_OUT" "$LAST_ERR"
    return ${exit_status}
}
